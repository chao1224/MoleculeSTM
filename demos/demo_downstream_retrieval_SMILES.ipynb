{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a41a864",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM Downstream: Structure-Text Retrieval\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bc8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:26:27,252] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from MoleculeSTM.datasets import DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "\n",
    "# Set-up the environment variable to ignore warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a7eee",
   "metadata": {},
   "source": [
    "## Setup Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d76596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, T_list=[4, 10, 20], batch_size=32, dataspace_path='../data', decay=0, device=0, epochs=1, eval_train=0, input_model_dir='demo_checkpoints_SMILES', input_model_path='demo_checkpoints_SMILES/molecule_model.pth', load_latent_projector=1, max_seq_len=512, mol_lr=1e-05, mol_lr_scale=0.1, molecule_type='SMILES', normalize=True, num_workers=8, seed=42, task='molecule_description', test_mode='given_text', text_lr=1e-05, text_lr_scale=0.1, text_type='SciBERT', training_mode='zero_shot', verbose=0, vocab_path='../MoleculeSTM/bart_vocab.txt')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\", \"BioBERT\"])\n",
    "parser.add_argument(\"--load_latent_projector\", type=int, default=1)\n",
    "parser.add_argument(\"--training_mode\", type=str, default=\"zero_shot\", choices=[\"zero_shot\"])\n",
    "\n",
    "########## for dataset and split ##########\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--task\", type=str, default=\"molecule_description\",\n",
    "    choices=[\n",
    "        \"molecule_description\", \"molecule_description_Raw\",\n",
    "        \"molecule_description_removed_PubChem\", \"molecule_description_removed_PubChem_Raw\",\n",
    "        \"molecule_pharmacodynamics\", \"molecule_pharmacodynamics_Raw\",\n",
    "        \"molecule_pharmacodynamics_removed_PubChem\", \"molecule_pharmacodynamics_removed_PubChem_Raw\"])\n",
    "parser.add_argument(\"--test_mode\", type=str, default=\"given_text\", choices=[\"given_text\", \"given_molecule\"])\n",
    "\n",
    "########## for optimization ##########\n",
    "parser.add_argument(\"--T_list\", type=int, nargs=\"+\", default=[4, 10, 20])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "\n",
    "########## for contrastive objective ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "########## for BERT model ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for molecule model ##########\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"SMILES\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--vocab_path\", type=str, default=\"../MoleculeSTM/bart_vocab.txt\")\n",
    "\n",
    "########## for saver ##########\n",
    "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--input_model_dir\", type=str, default=\"demo_checkpoints_SMILES\")\n",
    "parser.add_argument(\"--input_model_path\", type=str, default=\"demo_checkpoints_SMILES/molecule_model.pth\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f80fc0",
   "metadata": {},
   "source": [
    "## Setup Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7cb65",
   "metadata": {},
   "source": [
    "## Load SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e70ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/text_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "text_dim = 768\n",
    "\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99247bc",
   "metadata": {},
   "source": [
    "## Load MoleculeSTM-SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4964eb40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/molecule_model.pth...\n",
      "using world size: 1 and model-parallel size: 1 \n",
      "using torch.float32 for parameters ...\n",
      "-------------------- arguments --------------------\n",
      "  adam_beta1 ...................... 0.9\n",
      "  adam_beta2 ...................... 0.999\n",
      "  adam_eps ........................ 1e-08\n",
      "  adlr_autoresume ................. False\n",
      "  adlr_autoresume_interval ........ 1000\n",
      "  apply_query_key_layer_scaling ... False\n",
      "  apply_residual_connection_post_layernorm  False\n",
      "  attention_dropout ............... 0.1\n",
      "  attention_softmax_in_fp32 ....... False\n",
      "  batch_size ...................... None\n",
      "  bert_load ....................... None\n",
      "  bias_dropout_fusion ............. False\n",
      "  bias_gelu_fusion ................ False\n",
      "  block_data_path ................. None\n",
      "  checkpoint_activations .......... False\n",
      "  checkpoint_in_cpu ............... False\n",
      "  checkpoint_num_layers ........... 1\n",
      "  clip_grad ....................... 1.0\n",
      "  contigious_checkpointing ........ False\n",
      "  cpu_optimizer ................... False\n",
      "  cpu_torch_adam .................. False\n",
      "  data_impl ....................... infer\n",
      "  data_path ....................... None\n",
      "  dataset_path .................... None\n",
      "  DDP_impl ........................ local\n",
      "  deepscale ....................... False\n",
      "  deepscale_config ................ None\n",
      "  deepspeed ....................... False\n",
      "  deepspeed_activation_checkpointing  False\n",
      "  deepspeed_config ................ None\n",
      "  deepspeed_mpi ................... False\n",
      "  distribute_checkpointed_activations  False\n",
      "  distributed_backend ............. nccl\n",
      "  dynamic_loss_scale .............. True\n",
      "  eod_mask_loss ................... False\n",
      "  eval_interval ................... 1000\n",
      "  eval_iters ...................... 100\n",
      "  exit_interval ................... None\n",
      "  faiss_use_gpu ................... False\n",
      "  finetune ........................ False\n",
      "  fp16 ............................ False\n",
      "  fp16_lm_cross_entropy ........... False\n",
      "  fp32_allreduce .................. False\n",
      "  gas ............................. 1\n",
      "  hidden_dropout .................. 0.1\n",
      "  hidden_size ..................... 256\n",
      "  hysteresis ...................... 2\n",
      "  ict_head_size ................... None\n",
      "  ict_load ........................ None\n",
      "  indexer_batch_size .............. 128\n",
      "  indexer_log_interval ............ 1000\n",
      "  init_method_std ................. 0.02\n",
      "  layernorm_epsilon ............... 1e-05\n",
      "  lazy_mpu_init ................... None\n",
      "  load ............................ None\n",
      "  local_rank ...................... None\n",
      "  log_interval .................... 100\n",
      "  loss_scale ...................... None\n",
      "  loss_scale_window ............... 1000\n",
      "  lr .............................. None\n",
      "  lr_decay_iters .................. None\n",
      "  lr_decay_style .................. linear\n",
      "  make_vocab_size_divisible_by .... 128\n",
      "  mask_prob ....................... 0.15\n",
      "  max_position_embeddings ......... 512\n",
      "  merge_file ...................... None\n",
      "  min_lr .......................... 0.0\n",
      "  min_scale ....................... 1\n",
      "  mmap_warmup ..................... False\n",
      "  model_parallel_size ............. 1\n",
      "  no_load_optim ................... False\n",
      "  no_load_rng ..................... False\n",
      "  no_save_optim ................... False\n",
      "  no_save_rng ..................... False\n",
      "  num_attention_heads ............. 8\n",
      "  num_layers ...................... 4\n",
      "  num_unique_layers ............... None\n",
      "  num_workers ..................... 2\n",
      "  onnx_safe ....................... None\n",
      "  openai_gelu ..................... False\n",
      "  override_lr_scheduler ........... False\n",
      "  param_sharing_style ............. grouped\n",
      "  params_dtype .................... torch.float32\n",
      "  partition_activations ........... False\n",
      "  pipe_parallel_size .............. 0\n",
      "  profile_backward ................ False\n",
      "  query_in_block_prob ............. 0.1\n",
      "  rank ............................ 0\n",
      "  report_topk_accuracies .......... []\n",
      "  reset_attention_mask ............ False\n",
      "  reset_position_ids .............. False\n",
      "  save ............................ None\n",
      "  save_interval ................... None\n",
      "  scaled_masked_softmax_fusion .... False\n",
      "  scaled_upper_triang_masked_softmax_fusion  False\n",
      "  seed ............................ 1234\n",
      "  seq_length ...................... None\n",
      "  short_seq_prob .................. 0.1\n",
      "  split ........................... 969, 30, 1\n",
      "  synchronize_each_layer .......... False\n",
      "  tensorboard_dir ................. None\n",
      "  titles_data_path ................ None\n",
      "  tokenizer_type .................. GPT2BPETokenizer\n",
      "  train_iters ..................... None\n",
      "  use_checkpoint_lr_scheduler ..... False\n",
      "  use_cpu_initialization .......... False\n",
      "  use_one_sent_docs ............... False\n",
      "  vocab_file ...................... ../MoleculeSTM/bart_vocab.txt\n",
      "  warmup .......................... 0.01\n",
      "  weight_decay .................... 0.01\n",
      "  world_size ...................... 1\n",
      "  zero_allgather_bucket_size ...... 0.0\n",
      "  zero_contigious_gradients ....... False\n",
      "  zero_reduce_bucket_size ......... 0.0\n",
      "  zero_reduce_scatter ............. False\n",
      "  zero_stage ...................... 1.0\n",
      "---------------- end of arguments ----------------\n",
      "> initializing torch distributed ...\n",
      "> initializing model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "Loading vocab from ../MoleculeSTM/bart_vocab.txt.\n"
     ]
    }
   ],
   "source": [
    "input_model_path = os.path.join(args.input_model_dir, \"molecule_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "MegaMolBART_wrapper = MegaMolBART(vocab_path=args.vocab_path, input_dir=None, output_dir=None)\n",
    "molecule_model = MegaMolBART_wrapper.model\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "molecule_model.load_state_dict(state_dict)\n",
    "molecule_dim = 256\n",
    "\n",
    "# Rewrite the seed by MegaMolBART\n",
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4a0cf",
   "metadata": {},
   "source": [
    "## Load Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d28fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_SMILES/text2latent_model.pth...\n",
      "Loading from demo_checkpoints_SMILES/mol2latent_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text2latent.load_state_dict(state_dict)\n",
    "\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"mol2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "mol2latent.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cd050",
   "metadata": {},
   "source": [
    "## Define Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146e5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL_eval(X, Y, neg_Y, args):\n",
    "    X = F.normalize(X, dim=-1)\n",
    "    X = X.unsqueeze(1) # B, 1, d\n",
    "\n",
    "    Y = Y.unsqueeze(0)\n",
    "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
    "    Y = Y.transpose(0, 1)  # B, T, d\n",
    "    Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
    "    B = X.size()[0]\n",
    "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    CL_loss = criterion(logits, labels)\n",
    "    pred = logits.argmax(dim=1, keepdim=False)\n",
    "    confidence = logits\n",
    "    CL_conf = confidence.max(dim=1)[0]\n",
    "    CL_conf = CL_conf.cpu().numpy()\n",
    "\n",
    "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "    return CL_loss, CL_conf, CL_acc\n",
    "\n",
    "\n",
    "def get_text_repr(text):\n",
    "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
    "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
    "    text_repr = text_output[\"pooler_output\"]\n",
    "    text_repr = text2latent(text_repr)\n",
    "    return text_repr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(dataloader):\n",
    "    text_model.eval()\n",
    "    molecule_model.eval()\n",
    "    text2latent.eval()\n",
    "    mol2latent.eval()\n",
    "\n",
    "    accum_acc_list = [0 for _ in args.T_list]\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    for batch in L:\n",
    "        text = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "        neg_text = batch[2]\n",
    "        neg_molecule_data = batch[3]\n",
    "\n",
    "        text_repr = get_text_repr(text)\n",
    "        SMILES_list = list(molecule_data)\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            SMILES_list, mol2latent=mol2latent,\n",
    "            molecule_type=\"SMILES\", MegaMolBART_wrapper=MegaMolBART_wrapper)\n",
    "\n",
    "        if test_mode == \"given_text\":\n",
    "            neg_molecule_repr = [\n",
    "                get_molecule_repr_MoleculeSTM(\n",
    "                    list(neg_molecule_data[idx]), mol2latent=mol2latent,\n",
    "                    molecule_type=\"SMILES\", MegaMolBART_wrapper=MegaMolBART_wrapper) for idx in range(T_max)\n",
    "            ]\n",
    "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
    "\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        elif test_mode == \"given_molecule\":\n",
    "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
    "            neg_text_repr = torch.stack(neg_text_repr)\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        else:\n",
    "            raise Exception\n",
    "    \n",
    "    accum_acc_list = np.array(accum_acc_list)\n",
    "    accum_acc_list /= len(dataloader)\n",
    "    return accum_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b41532",
   "metadata": {},
   "source": [
    "## Start Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading negative samples from ../data/DrugBank_data/index/SMILES_description_full.txt\n",
      "Results [0.94256757 0.89864865 0.84797297]\n"
     ]
    }
   ],
   "source": [
    "text_model = text_model.to(device)\n",
    "molecule_model = molecule_model.to(device)\n",
    "text2latent = text2latent.to(device)\n",
    "mol2latent = mol2latent.to(device)\n",
    "\n",
    "T_max = max(args.T_list) - 1\n",
    "\n",
    "initial_test_acc_list = []\n",
    "test_mode = args.test_mode\n",
    "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
    "\n",
    "dataset_class = DrugBank_Datasets_SMILES_retrieval\n",
    "dataloader_class = torch_DataLoader\n",
    "\n",
    "if args.task == \"molecule_description\":\n",
    "    template = \"SMILES_description_{}.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_description_Raw\":\n",
    "    template = \"SMILES_description_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics\":\n",
    "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
    "\n",
    "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, template=template)\n",
    "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
    "\n",
    "initial_test_acc_list = eval_epoch(full_dataloader)\n",
    "print('Results', initial_test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
