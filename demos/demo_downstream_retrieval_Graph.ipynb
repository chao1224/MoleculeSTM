{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a41a864",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM Downstream: Structure-Text Retrieval\n",
    "\n",
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9bc8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-30 12:24:25,704] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from MoleculeSTM.datasets import DrugBank_Datasets_SMILES_retrieval, DrugBank_Datasets_Graph_retrieval\n",
    "from MoleculeSTM.models.mega_molbart.mega_mol_bart import MegaMolBART\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network\n",
    "\n",
    "# Set-up the environment variable to ignore warnings\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a7eee",
   "metadata": {},
   "source": [
    "## Setup Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2d76596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, JK='last', SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, T_list=[4, 10, 20], batch_size=32, dataset='PubChem', dataspace_path='../data', decay=0, device=0, dropout_ratio=0.5, epochs=1, eval_train=0, gnn_emb_dim=300, gnn_type='gin', graph_pooling='mean', input_model_dir='demo_checkpoints_Graph', input_model_path='demo_checkpoints_Graph/molecule_model.pth', load_latent_projector=1, max_seq_len=512, mol_lr=1e-05, mol_lr_scale=0.1, molecule_type='Graph', normalize=True, num_layer=5, num_workers=8, seed=42, task='molecule_description', test_mode='given_text', text_lr=1e-05, text_lr_scale=0.1, text_type='SciBERT', training_mode='zero_shot', verbose=0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\", \"BioBERT\"])\n",
    "parser.add_argument(\"--load_latent_projector\", type=int, default=1)\n",
    "parser.add_argument(\"--training_mode\", type=str, default=\"zero_shot\", choices=[\"zero_shot\"])\n",
    "\n",
    "########## for dataset and split ##########\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PubChem\")\n",
    "parser.add_argument(\"--task\", type=str, default=\"molecule_description\",\n",
    "    choices=[\n",
    "        \"molecule_description\", \"molecule_description_Raw\",\n",
    "        \"molecule_description_removed_PubChem\", \"molecule_description_removed_PubChem_Raw\",\n",
    "        \"molecule_pharmacodynamics\", \"molecule_pharmacodynamics_Raw\",\n",
    "        \"molecule_pharmacodynamics_removed_PubChem\", \"molecule_pharmacodynamics_removed_PubChem_Raw\"])\n",
    "parser.add_argument(\"--test_mode\", type=str, default=\"given_text\", choices=[\"given_text\", \"given_molecule\"])\n",
    "\n",
    "########## for optimization ##########\n",
    "parser.add_argument(\"--T_list\", type=int, nargs=\"+\", default=[4, 10, 20])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "\n",
    "########## for contrastive objective ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "########## for BERT model ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for molecule model ##########\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "########## for 2D GNN ##########\n",
    "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
    "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
    "parser.add_argument('--JK', type=str, default='last')\n",
    "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
    "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
    "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
    "\n",
    "########## for saver ##########\n",
    "parser.add_argument(\"--eval_train\", type=int, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--input_model_dir\", type=str, default=\"demo_checkpoints_Graph\")\n",
    "parser.add_argument(\"--input_model_path\", type=str, default=\"demo_checkpoints_Graph/molecule_model.pth\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f80fc0",
   "metadata": {},
   "source": [
    "## Setup Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.random.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7cb65",
   "metadata": {},
   "source": [
    "## Load SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8e70ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_Graph/text_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "text_dim = 768\n",
    "\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99247bc",
   "metadata": {},
   "source": [
    "## Load MoleculeSTM-Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4964eb40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_Graph/molecule_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "molecule_node_model = GNN(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
    "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
    "    gnn_type=args.gnn_type)\n",
    "molecule_model = GNN_graphpred(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
    "    num_tasks=1, molecule_node_model=molecule_node_model) \n",
    "molecule_dim = args.gnn_emb_dim\n",
    "\n",
    "input_model_path = os.path.join(args.input_model_dir, \"molecule_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "molecule_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4a0cf",
   "metadata": {},
   "source": [
    "## Load Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d28fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from demo_checkpoints_Graph/text2latent_model.pth...\n",
      "Loading from demo_checkpoints_Graph/mol2latent_model.pth...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"text2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "text2latent.load_state_dict(state_dict)\n",
    "\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim)\n",
    "input_model_path = os.path.join(args.input_model_dir, \"mol2latent_model.pth\")\n",
    "print(\"Loading from {}...\".format(input_model_path))\n",
    "state_dict = torch.load(input_model_path, map_location='cpu')\n",
    "mol2latent.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cd050",
   "metadata": {},
   "source": [
    "## Define Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146e5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL_eval(X, Y, neg_Y, args):\n",
    "    X = F.normalize(X, dim=-1)\n",
    "    X = X.unsqueeze(1) # B, 1, d\n",
    "\n",
    "    Y = Y.unsqueeze(0)\n",
    "    Y = torch.cat([Y, neg_Y], dim=0) # T, B, d\n",
    "    Y = Y.transpose(0, 1)  # B, T, d\n",
    "    Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    logits = torch.bmm(X, Y.transpose(1, 2)).squeeze()  # B*T\n",
    "    B = X.size()[0]\n",
    "    labels = torch.zeros(B).long().to(logits.device)  # B*1\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    CL_loss = criterion(logits, labels)\n",
    "    pred = logits.argmax(dim=1, keepdim=False)\n",
    "    confidence = logits\n",
    "    CL_conf = confidence.max(dim=1)[0]\n",
    "    CL_conf = CL_conf.cpu().numpy()\n",
    "\n",
    "    CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "    return CL_loss, CL_conf, CL_acc\n",
    "\n",
    "\n",
    "def get_text_repr(text):\n",
    "    text_tokens_ids, text_masks = prepare_text_tokens(\n",
    "        device=device, description=text, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "    text_output = text_model(input_ids=text_tokens_ids, attention_mask=text_masks)\n",
    "    text_repr = text_output[\"pooler_output\"]\n",
    "    text_repr = text2latent(text_repr)\n",
    "    return text_repr\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(dataloader):\n",
    "    text_model.eval()\n",
    "    molecule_model.eval()\n",
    "    text2latent.eval()\n",
    "    mol2latent.eval()\n",
    "\n",
    "    accum_acc_list = [0 for _ in args.T_list]\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    for batch in L:\n",
    "        text = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "        neg_text = batch[2]\n",
    "        neg_molecule_data = batch[3]\n",
    "\n",
    "        text_repr = get_text_repr(text)\n",
    "\n",
    "        molecule_data = molecule_data.to(device)\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            molecule_data, mol2latent=mol2latent,\n",
    "            molecule_type=\"Graph\", molecule_model=molecule_model)\n",
    "\n",
    "        if test_mode == \"given_text\":\n",
    "            neg_molecule_repr = [\n",
    "                get_molecule_repr_MoleculeSTM(\n",
    "                    neg_molecule_data[idx].to(device), mol2latent=mol2latent,\n",
    "                    molecule_type=\"Graph\", molecule_model=molecule_model) for idx in range(T_max)\n",
    "            ]\n",
    "            neg_molecule_repr = torch.stack(neg_molecule_repr)\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(text_repr, molecule_repr, neg_molecule_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        elif test_mode == \"given_molecule\":\n",
    "            neg_text_repr = [get_text_repr(neg_text[idx]) for idx in range(T_max)]\n",
    "            neg_text_repr = torch.stack(neg_text_repr)\n",
    "            for T_idx, T in enumerate(args.T_list):\n",
    "                _, _, acc = do_CL_eval(molecule_repr, text_repr, neg_text_repr[:T-1], args)\n",
    "                accum_acc_list[T_idx] += acc\n",
    "        else:\n",
    "            raise Exception\n",
    "    \n",
    "    accum_acc_list = np.array(accum_acc_list)\n",
    "    accum_acc_list /= len(dataloader)\n",
    "    return accum_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b41532",
   "metadata": {},
   "source": [
    "## Start Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfb842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: Data(x=[40309, 2], edge_index=[2, 85886], edge_attr=[85886, 2], id=[1168])\n",
      "Index(['text', 'smiles'], dtype='object')\n",
      "Loading negative samples from ../data/DrugBank_data/index/SMILES_description_full.txt\n",
      "Results [0.96030405 0.9214527  0.87584459]\n"
     ]
    }
   ],
   "source": [
    "text_model = text_model.to(device)\n",
    "molecule_model = molecule_model.to(device)\n",
    "text2latent = text2latent.to(device)\n",
    "mol2latent = mol2latent.to(device)\n",
    "\n",
    "T_max = max(args.T_list) - 1\n",
    "\n",
    "initial_test_acc_list = []\n",
    "test_mode = args.test_mode\n",
    "dataset_folder = os.path.join(args.dataspace_path, \"DrugBank_data\")\n",
    "\n",
    "\n",
    "dataset_class = DrugBank_Datasets_Graph_retrieval\n",
    "dataloader_class = pyg_DataLoader\n",
    "processed_dir_prefix = args.task\n",
    "\n",
    "if args.task == \"molecule_description\":\n",
    "    template = \"SMILES_description_{}.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_description_Raw\":\n",
    "    template = \"SMILES_description_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_description_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_description_removed_from_PubChem_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics\":\n",
    "    template = \"SMILES_pharmacodynamics_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_{}_Raw.txt\"\n",
    "elif args.task == \"molecule_pharmacodynamics_removed_PubChem_Raw\":\n",
    "    template = \"SMILES_pharmacodynamics_removed_from_PubChem_{}_Raw.txt\"\n",
    "\n",
    "full_dataset = dataset_class(dataset_folder, 'full', neg_sample_size=T_max, processed_dir_prefix=processed_dir_prefix, template=template)\n",
    "full_dataloader = dataloader_class(full_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers) # The program will get blcoked with none-zero num_workers\n",
    "\n",
    "initial_test_acc_list = eval_epoch(full_dataloader)\n",
    "print('Results', initial_test_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
