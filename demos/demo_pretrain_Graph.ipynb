{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for MoleculeSTM pretraining\n",
    "\n",
    "All the scripts can be found in `MoleculeSTM/pretrain.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load and Customize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arguments\t Namespace(CL_neg_samples=1, JK='last', SSL_emb_dim=256, SSL_loss='EBM_NCE', T=0.1, batch_size=4, dataset='PubChemSTM1K', dataspace_path='../data', decay=0, device=0, dropout_ratio=0.5, epochs=100, gnn_emb_dim=300, gnn_type='gin', graph_pooling='mean', max_seq_len=512, megamolbart_input_dir='../data/pretrained_MegaMolBART/checkpoints', mol_lr=0.0001, mol_lr_scale=0.1, molecule_type='Graph', normalize=True, num_layer=5, num_workers=8, output_model_dir=None, pretrain_gnn_mode='GraphMVP_G', seed=42, text_lr=0.0001, text_lr_scale=0.1, text_type='SciBERT', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# Set-up the environment variable to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'False'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"../data\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"PubChemSTM1K\")\n",
    "parser.add_argument(\"--text_type\", type=str, default=\"SciBERT\", choices=[\"SciBERT\"])\n",
    "parser.add_argument(\"--molecule_type\", type=str, default=\"Graph\", choices=[\"SMILES\", \"Graph\"])\n",
    "\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--text_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--mol_lr\", type=float, default=1e-4)\n",
    "parser.add_argument(\"--text_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--mol_lr_scale\", type=float, default=0.1)\n",
    "parser.add_argument(\"--num_workers\", type=int, default=8)\n",
    "parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "parser.add_argument(\"--decay\", type=float, default=0)\n",
    "parser.add_argument(\"--verbose\", type=int, default=1)\n",
    "parser.add_argument(\"--output_model_dir\", type=str, default=None)\n",
    "\n",
    "########## for SciBERT ##########\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "########## for MegaMolBART ##########\n",
    "parser.add_argument(\"--megamolbart_input_dir\", type=str, default=\"../data/pretrained_MegaMolBART/checkpoints\")\n",
    "\n",
    "########## for 2D GNN ##########\n",
    "parser.add_argument(\"--pretrain_gnn_mode\", type=str, default=\"GraphMVP_G\", choices=[\"GraphMVP_G\"])\n",
    "parser.add_argument(\"--gnn_emb_dim\", type=int, default=300)\n",
    "parser.add_argument(\"--num_layer\", type=int, default=5)\n",
    "parser.add_argument('--JK', type=str, default='last')\n",
    "parser.add_argument(\"--dropout_ratio\", type=float, default=0.5)\n",
    "parser.add_argument(\"--gnn_type\", type=str, default=\"gin\")\n",
    "parser.add_argument('--graph_pooling', type=str, default='mean')\n",
    "\n",
    "########## for contrastive SSL ##########\n",
    "parser.add_argument(\"--SSL_loss\", type=str, default=\"EBM_NCE\", choices=[\"EBM_NCE\", \"InfoNCE\"])\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--CL_neg_samples\", type=int, default=1)\n",
    "parser.add_argument(\"--T\", type=float, default=0.1)\n",
    "parser.add_argument('--normalize', dest='normalize', action='store_true')\n",
    "parser.add_argument('--no_normalize', dest='normalize', action='store_false')\n",
    "parser.set_defaults(normalize=True)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"arguments\\t\", args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as torch_DataLoader\n",
    "\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from MoleculeSTM.datasets import (\n",
    "    PubChemSTM_Datasets_SMILES, PubChemSTM_SubDatasets_SMILES,\n",
    "    PubChemSTM_Datasets_Graph, PubChemSTM_SubDatasets_Graph,\n",
    "    PubChemSTM_Datasets_Raw_SMILES, PubChemSTM_SubDatasets_Raw_SMILES,\n",
    "    PubChemSTM_Datasets_Raw_Graph, PubChemSTM_SubDatasets_Raw_Graph\n",
    ")\n",
    "from MoleculeSTM.models import GNN, GNN_graphpred\n",
    "from MoleculeSTM.utils import prepare_text_tokens, get_molecule_repr_MoleculeSTM, freeze_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_index(num, shift):\n",
    "    arr = torch.arange(num) + shift\n",
    "    arr[-shift:] = torch.arange(shift)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def do_CL(X, Y, args):\n",
    "    if args.normalize:\n",
    "        X = F.normalize(X, dim=-1)\n",
    "        Y = F.normalize(Y, dim=-1)\n",
    "\n",
    "    if args.SSL_loss == 'EBM_NCE':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        neg_Y = torch.cat([Y[cycle_index(len(Y), i + 1)] for i in range(args.CL_neg_samples)], dim=0)\n",
    "        neg_X = X.repeat((args.CL_neg_samples, 1))\n",
    "\n",
    "        pred_pos = torch.sum(X * Y, dim=1) / args.T\n",
    "        pred_neg = torch.sum(neg_X * neg_Y, dim=1) / args.T\n",
    "\n",
    "        loss_pos = criterion(pred_pos, torch.ones(len(pred_pos)).to(pred_pos.device))\n",
    "        loss_neg = criterion(pred_neg, torch.zeros(len(pred_neg)).to(pred_neg.device))\n",
    "        CL_loss = (loss_pos + args.CL_neg_samples * loss_neg) / (1 + args.CL_neg_samples)\n",
    "\n",
    "        CL_acc = (torch.sum(pred_pos > 0).float() + torch.sum(pred_neg < 0).float()) / \\\n",
    "                 (len(pred_pos) + len(pred_neg))\n",
    "        CL_acc = CL_acc.detach().cpu().item()\n",
    "\n",
    "    elif args.SSL_loss == 'InfoNCE':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        B = X.size()[0]\n",
    "        logits = torch.mm(X, Y.transpose(1, 0))  # B*B\n",
    "        logits = torch.div(logits, args.T)\n",
    "        labels = torch.arange(B).long().to(logits.device)  # B*1\n",
    "\n",
    "        CL_loss = criterion(logits, labels)\n",
    "        pred = logits.argmax(dim=1, keepdim=False)\n",
    "        CL_acc = pred.eq(labels).sum().detach().cpu().item() * 1. / B\n",
    "\n",
    "    else:\n",
    "        raise Exception\n",
    "\n",
    "    return CL_loss, CL_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch,\n",
    "    dataloader,\n",
    "    text_model, text_tokenizer,\n",
    "    molecule_model, MegaMolBART_wrapper=None):\n",
    "\n",
    "    text_model.train()\n",
    "    molecule_model.train()\n",
    "    text2latent.train()\n",
    "    mol2latent.train()\n",
    "\n",
    "    if args.verbose:\n",
    "        L = tqdm(dataloader)\n",
    "    else:\n",
    "        L = dataloader\n",
    "    \n",
    "    start_time = time.time()\n",
    "    accum_loss, accum_acc = 0, 0\n",
    "    for step, batch in enumerate(L):\n",
    "        description = batch[0]\n",
    "        molecule_data = batch[1]\n",
    "\n",
    "        description_tokens_ids, description_masks = prepare_text_tokens(\n",
    "            device=device, description=description, tokenizer=text_tokenizer, max_seq_len=args.max_seq_len)\n",
    "        description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "        description_repr = description_output[\"pooler_output\"]\n",
    "        description_repr = text2latent(description_repr)\n",
    "\n",
    "        molecule_data = molecule_data.to(device)\n",
    "        molecule_repr = get_molecule_repr_MoleculeSTM(\n",
    "            molecule_data, mol2latent=mol2latent,\n",
    "            molecule_type=molecule_type, molecule_model=molecule_model)\n",
    "\n",
    "        loss_01, acc_01 = do_CL(description_repr, molecule_repr, args)\n",
    "        loss_02, acc_02 = do_CL(molecule_repr, description_repr, args)\n",
    "        loss = (loss_01 + loss_02) / 2\n",
    "        acc = (acc_01 + acc_02) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accum_loss += loss.item()\n",
    "        accum_acc += acc\n",
    "    \n",
    "    accum_loss /= len(L)\n",
    "    accum_acc /= len(L)\n",
    "    \n",
    "    global optimal_loss\n",
    "    temp_loss = accum_loss\n",
    "    if temp_loss < optimal_loss:\n",
    "        optimal_loss = temp_loss\n",
    "    print(\"CL Loss: {:.5f}\\tCL Acc: {:.5f}\\tTime: {:.5f}\".format(accum_loss, accum_acc, time.time() - start_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Start Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "device = torch.device(\"cuda:\" + str(args.device)) \\\n",
    "    if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Text Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download SciBert to ../data/pretrained_SciBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "kwargs = {}\n",
    "\n",
    "if args.text_type == \"SciBERT\":\n",
    "    pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "    print(\"Download SciBert to {}\".format(pretrained_SciBERT_folder))\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "    text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)\n",
    "    kwargs[\"text_tokenizer\"] = text_tokenizer\n",
    "    kwargs[\"text_model\"] = text_model\n",
    "    text_dim = 768\n",
    "else:\n",
    "    raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Start training MoleculeSTM-Graph\n",
    "\n",
    "#### 5.3.1 Prepare GraphMVP (Graph Model) and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data/pretrained_GraphMVP/GraphMVP_G/model.pth ...\n"
     ]
    }
   ],
   "source": [
    "dataset_root = os.path.join(args.dataspace_path, \"PubChemSTM_data\")\n",
    "    \n",
    "molecule_type = \"Graph\"\n",
    "\n",
    "# You need to first run the following for data preprocessing if you haven't done so.\n",
    "# PubChemSTM_Datasets_Graph(dataset_root)\n",
    "dataset = PubChemSTM_SubDatasets_Graph(dataset_root, size=1000)\n",
    "\n",
    "dataloader_class = pyg_DataLoader\n",
    "\n",
    "molecule_node_model = GNN(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim,\n",
    "    JK=args.JK, drop_ratio=args.dropout_ratio,\n",
    "    gnn_type=args.gnn_type)\n",
    "molecule_model = GNN_graphpred(\n",
    "    num_layer=args.num_layer, emb_dim=args.gnn_emb_dim, JK=args.JK, graph_pooling=args.graph_pooling,\n",
    "    num_tasks=1, molecule_node_model=molecule_node_model)\n",
    "pretrained_model_path = os.path.join(args.dataspace_path, \"pretrained_GraphMVP\", args.pretrain_gnn_mode, \"model.pth\")\n",
    "molecule_model.from_pretrained(pretrained_model_path)\n",
    "\n",
    "molecule_model = molecule_model.to(device)\n",
    "\n",
    "kwargs[\"molecule_model\"] = molecule_model\n",
    "molecule_dim = args.gnn_emb_dim\n",
    "\n",
    "dataloader = dataloader_class(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Prepare Two Projection Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text2latent = nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "mol2latent = nn.Linear(molecule_dim, args.SSL_emb_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Prepare Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param_group = [\n",
    "    {\"params\": text_model.parameters(), \"lr\": args.text_lr},\n",
    "    {\"params\": molecule_model.parameters(), \"lr\": args.mol_lr},\n",
    "    {\"params\": text2latent.parameters(), \"lr\": args.text_lr * args.text_lr_scale},\n",
    "    {\"params\": mol2latent.parameters(), \"lr\": args.mol_lr * args.mol_lr_scale},\n",
    "]\n",
    "optimizer = optim.Adam(model_param_group, weight_decay=args.decay)\n",
    "optimal_loss = 1e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:57<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.71635\tCL Acc: 0.50225\tTime: 57.53959\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:56<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.70258\tCL Acc: 0.49950\tTime: 56.35668\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:56<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL Loss: 0.69960\tCL Acc: 0.49900\tTime: 56.90493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for e in range(3):\n",
    "    print(\"Epoch {}\".format(e))\n",
    "    train(e, dataloader, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
